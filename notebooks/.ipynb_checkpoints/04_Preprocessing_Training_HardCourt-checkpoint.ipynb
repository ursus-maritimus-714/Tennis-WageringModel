{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dcb9885",
   "metadata": {},
   "source": [
    "### Introduction to Preprocessing and Training Stage (Hard Court Version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbf4c49",
   "metadata": {},
   "source": [
    "The goal of this stage is to create a training-testing split in the predictive features and target feature (% total points won by a given player in a given match), and to create a few simple benchmark models against which the more complex models will be compared in terms of prediction accuracy. \n",
    "\n",
    "Prediction is to be carried out at the player level for a given match (so each match in the sample has two records associated with it). Performance features used for prediction of a given player's % pts won in a given match are accrued based on surface-specific match data in matches PRIOR TO BUT NOT INCLDING the match being predicted on (NOR including any matches player AFTER the match being predicted on). \n",
    "\n",
    "\"Raw\" features are specific to one of the two players in a given match (eg, log of ranking, height, handedness, time decay-weighted past serve or return points won%, decay-weighted time on court previously in the same tournament), or represent metadata pertinent to player-independent aspects of a match (eg, surface, court speed, location) . \n",
    "\n",
    "\"Differential\" features are derivatives of the \"raw\" features where a subtraction is done between the two players in the match being predicted on (Player X - Player Y or Player Y - Player X). For example, if Player X won 55% of his serve points (time-decay weighted) in his prior 60 matches on the same surface as the match to be predicted on, and his opponent Player Y won 47% of his own serve points (time-decay weighted) in his prior 60 matches on the same surface, the \"differential\" feature value \"p_pts_sv_won_l60_decay_diff\" for Player X would be 55 - 47 = 8% and for Player Y would be 47 - 55 = -8%. The according \"raw\" feature values, in turn, \"p_pts_sv_won_l60_decay\" would be 55% and 47% for Player X and Player Y, respectively. \n",
    "\n",
    "Currently, data (on a surface-specific basis) from years 2015-2019 are used in EDA/modeling for hard court matches (2012-2019 for the smaller sample size/year clay court match sample), with an additional 3 years prior to that used for retrospective feature accrual (2009-2011 for clay court matches, 2012-2014 for hard court matches). Also, a threshold of minimum of 20 prior matches for BOTH players in a given match to be predicted on is employed. Not surprisingly, prediction accuracy is sensitive to the amount of data available to generate predictive features, as well as to the amount of data available to train and test the model. Critically, matches filtered out at the current point and beyond (ie, during modeling) WERE used for feature generation.\n",
    "    * As an additional data inclusion reminder, matches played on grass (too low a sample size; also removed Davis Cup and Olympics matches for same reason as well as for their \"odd\" contexts) and matches where one player withdrew (usually for injury reason) either before the match or early on (<12 games in) in the match were filtered out and NOT included in feature generation.    \n",
    "\n",
    "Two simple benchmarking models are created below:\n",
    "* A \"dummy\" model, which simply uses the mean of the training data split to predict the target of % points won by a given player in a given match\n",
    "* A simple multivariate linear model that utilizes only ATP ranking information, both raw ranking and differential ranking between the two players in the match being predicted on, to predict the target. Given that tournament entry and seeding are determined primarily by ranking data by the tour itself, this simple model can very prudently be seen as a fair benchmark for any system hoping to predict player performance on a move-forward basis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fc727e",
   "metadata": {},
   "source": [
    "## Preprocessing and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e179e73d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'library'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-5529c01e509e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSelectKBest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf_regression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mlibrary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msb_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msave_file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'library'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import __version__ as sklearn_version\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV, learning_curve\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "import datetime\n",
    "from library.sb_utils import save_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e352c49",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc5018d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/df_player_benchmark_hard.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95327794",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa480d1",
   "metadata": {},
   "source": [
    "### Filter Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708676d0",
   "metadata": {},
   "source": [
    "See notebook Intro for details and justification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62e79ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filter = df[~df['tour_wk'].str.contains(\"2012\")] \n",
    "df_filter = df_filter[~df_filter['tour_wk'].str.contains(\"2013\")]\n",
    "df_filter = df_filter[~df_filter['tour_wk'].str.contains(\"2014\")]\n",
    "#df_filter = df_filter[~df_filter['tour_wk'].str.contains(\"2015\")]\n",
    "#df_filter = df_filter[~df_filter['tour_wk'].str.contains(\"2016\")]\n",
    "#df_filter = df_filter[~df_filter['tour_wk'].str.contains(\"2017\")]\n",
    "#df_filter = df_filter[~df_filter['tour_wk'].str.contains(\"2018\")]\n",
    "#df_filter = df_filter[~df_filter['tour_wk'].str.contains(\"2019\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ad0d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter down to only matches played on hard courts \n",
    "df_filter2 = df_filter.loc[(df_filter[\"t_surf\"] == 2)]\n",
    "#df_filter2 = df_filter.loc[(df_filter[\"t_surf\"] == 2) & (df_filter[\"p_matches_surf\"] > 50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b299f363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now also will remove BOTH players from individual matches remaining in the surface-specific sample already filtered by year range\n",
    "# where one or both players has played N or fewer matches prior to the one to be predicted on. \n",
    "df_low = df_filter2.loc[df_filter2['p_matches_surf'] <= 20, 'm_num']\n",
    "df_filter3 = df_filter2[~df_filter2['m_num'].isin(df_low)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0b65fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filter3.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6164441",
   "metadata": {},
   "source": [
    "Stripping dataframe down to only the raw an differential rankings-based features we will need to generate a \"benchmarking\" linear model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a027a7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model1 = df_filter3[[\"p_pts_won%\", \"p_rank\", \"p_opp_rank_diff\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f787a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f76d820",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dfcba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_model1) * .75, len(df_model1) * .25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41207c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_model1.drop(columns='p_pts_won%'), \n",
    "                                                    df_model1[\"p_pts_won%\"], test_size=0.25, \n",
    "                                                    random_state=47)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aadc9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3da5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3944cffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9eb379",
   "metadata": {},
   "source": [
    "### Pre-Modeling: Mean Points Won% by a Given Player in Given Match as Predictor (aka \"Dummy Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e3929c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Target feature (p_pts_won%) training set mean\n",
    "train_mean = y_train.mean()\n",
    "train_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef94f8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting dummy regressor to training data (from sklearn). Outputs the training set mean.\n",
    "dumb_reg = DummyRegressor(strategy='mean')\n",
    "dumb_reg.fit(X_train, y_train)\n",
    "dumb_reg.constant_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20954f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr_pred = dumb_reg.predict(X_train)\n",
    "y_tr_pred[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682d3306",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_te_pred = train_mean * np.ones(len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f9a10a",
   "metadata": {},
   "source": [
    "#### R-Squared (COD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5db87e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute R-squared for target mean on training set (will be zero, since we are calculating mean on training set), and on test set (should be slightly different from zero)\n",
    "r2_score(y_train, y_tr_pred), r2_score(y_test, y_te_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d65bdb",
   "metadata": {},
   "source": [
    "proportion of the variance for a dependent variable that's explained by our features. It's close to zero for the dummy model as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b2e460",
   "metadata": {},
   "source": [
    "#### Mean Absolute Error (MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eac49b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAEs_dummy = mean_absolute_error(y_train, y_tr_pred), mean_absolute_error(y_test, y_te_pred)\n",
    "MAEs_dummy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e77ae7a",
   "metadata": {},
   "source": [
    "On average, we might expect to be off by around 5.19% on training set data and 5.21% on test data if you guessed a given player's percentage of points won in a given match based simply on an average of known values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8d0a0c",
   "metadata": {},
   "source": [
    "#### Mean Squared Error (MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce641104",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the Mean Squared Error (average of the square of the errors)\n",
    "MSEs_dummy = mean_squared_error(y_train, y_tr_pred), mean_squared_error(y_test, y_te_pred)\n",
    "MSEs_dummy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf480c4",
   "metadata": {},
   "source": [
    "#### Root Mean Squared Error (RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d96b979",
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSEs_dummy = np.sqrt(mean_squared_error(y_train, y_tr_pred)), np.sqrt(mean_squared_error(y_test, y_te_pred))\n",
    "RMSEs_dummy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7bd21b",
   "metadata": {},
   "source": [
    "The Dummy Model above is the ultimate straw man (we sure hope we can beat guessing with the average with all of the great data we have!). A slightly more fair comparison to our ultimate feature-rich model is a Linear Model based simply on the past match data provided by the ATP, namely player ranking and player ranking points. The ATP uses this data to decide on tournament entry status and seedings, so there is assuredly trust in these metrics by the governing body of tennis as far as evaluating players on tour. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a297aad7",
   "metadata": {},
   "source": [
    "### Linear Model Prediction With Player Ranking Data Only (Raw and Differential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dffb29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557cae51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter existing train-test split down to just the ranking columns (train)\n",
    "#X_train_ranking = X_train[[\"p_rank\", \"p_rank_pts\", \"p_log_rank\", \"p_opp_rank_diff\", \"p_opp_rank_pts_diff\", \"p_opp_log_rank_diff\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15bf91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter existing train-test split down to just the ranking columns (test)\n",
    "#X_test_ranking = X_test[[\"p_rank\", \"p_rank_pts\", \"p_log_rank\", \"p_opp_rank_diff\", \"p_opp_rank_pts_diff\", \"p_opp_log_rank_diff\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f05e589",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_pipe = make_pipeline(\n",
    "    SimpleImputer(strategy='median'), \n",
    "    StandardScaler(),\n",
    "    SelectKBest(f_regression),\n",
    "    LinearRegression()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6274eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dict of available parameters for linear regression pipe\n",
    "lr_pipe.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d45a3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define search grid parameters\n",
    "k = [k+1 for k in range(len(X_train.columns))]\n",
    "\n",
    "grid_params = {\n",
    "    'standardscaler': [StandardScaler(), None],\n",
    "    'simpleimputer__strategy': ['mean', 'median'],\n",
    "    'selectkbest__k': k\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fa5d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call `GridSearchCV` with linear regression pipeline, passing in the above `grid_params`\n",
    "#dict for parameters to evaluate with 5-fold cross-validation\n",
    "lr_grid_cv = GridSearchCV(lr_pipe, param_grid=grid_params, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1947d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conduct grid search for this ranking-restricted model. \n",
    "lr_grid_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1106123",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best params from grid search for this ranking-restricted model\n",
    "lr_grid_cv.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaaa954d",
   "metadata": {},
   "source": [
    "#### K Best Features Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3d0008",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_mean = lr_grid_cv.cv_results_['mean_test_score']\n",
    "score_std = lr_grid_cv.cv_results_['std_test_score']\n",
    "cv_k = [k for k in lr_grid_cv.cv_results_['param_selectkbest__k']]\n",
    "\n",
    "best_k = lr_grid_cv.best_params_['selectkbest__k']\n",
    "plt.subplots(figsize=(10, 5))\n",
    "plt.errorbar(cv_k, score_mean, yerr=score_std)\n",
    "plt.axvline(x=best_k, c='r', ls='--', alpha=.5)\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('CV score (r-squared)')\n",
    "plt.title('Pipeline mean CV score (error bars +/- 1sd)');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95754b7d",
   "metadata": {},
   "source": [
    "### Linear Model From Player Rankings and Player Ranking Differential Performance Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b934d9b",
   "metadata": {},
   "source": [
    "#### R-squared (COD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe2be9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross-validation defaults to R^2 metric for scoring regression\n",
    "lr_best_cv_results = cross_validate(lr_grid_cv.best_estimator_, X_train, y_train, cv=5)\n",
    "lr_best_scores = lr_best_cv_results['test_score']\n",
    "lr_best_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3d458a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training set CV mean and std\n",
    "np.mean(lr_best_scores), np.std(lr_best_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb63283c",
   "metadata": {},
   "source": [
    "#### Mean Absolute Error (MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d54fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_neg_mae = cross_validate(lr_grid_cv.best_estimator_, X_train, y_train, \n",
    "                            scoring='neg_mean_absolute_error', cv=5, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed58255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set MAE and STD \n",
    "lr_mae_mean = np.mean(-1 * lr_neg_mae['test_score'])\n",
    "lr_mae_std = np.std(-1 * lr_neg_mae['test_score'])\n",
    "MAE_LR_train = lr_mae_mean, lr_mae_std\n",
    "MAE_LR_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3142ce5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set mean\n",
    "MAE_LR_test = mean_absolute_error(y_test, lr_grid_cv.best_estimator_.predict(X_test))\n",
    "MAE_LR_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd96b16",
   "metadata": {},
   "source": [
    "#### Mean Squared Error (MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bb4408",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_neg_mse = cross_validate(lr_grid_cv.best_estimator_, X_train, y_train, \n",
    "                            scoring='neg_mean_squared_error', cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590c02f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training set CV mean and std\n",
    "lr_mse_mean = np.mean(-1 * lr_neg_mse['test_score'])\n",
    "lr_mse_std = np.std(-1 * lr_neg_mse['test_score'])\n",
    "MSE_LR_train = lr_mse_mean, lr_mse_std\n",
    "MSE_LR_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4b5820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set mean\n",
    "MSE_LR_test = mean_squared_error(y_test, lr_grid_cv.best_estimator_.predict(X_test))\n",
    "MSE_LR_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e903f84c",
   "metadata": {},
   "source": [
    "#### Root Mean Square Error (RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3400ccba",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_neg_rmse = cross_validate(lr_grid_cv.best_estimator_, X_train, y_train, \n",
    "                            scoring='neg_root_mean_squared_error', cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feef3271",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training set CV mean and std\n",
    "lr_rmse_mean = np.mean(-1 * lr_neg_rmse['test_score'])\n",
    "lr_rmse_std = np.std(-1 * lr_neg_rmse['test_score'])\n",
    "RMSE_LR_train = lr_rmse_mean, lr_rmse_std\n",
    "RMSE_LR_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd941f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set mean\n",
    "RMSE_LR_test = np.sqrt(mean_squared_error(y_test, lr_grid_cv.best_estimator_.predict(X_test)))\n",
    "RMSE_LR_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ab8c1c",
   "metadata": {},
   "source": [
    "### Best Linear Model Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45afdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plots a barplot of the linear regressor feature importances,\n",
    "#assigning the `feature_importances_` attribute of \n",
    "#`lv_grid_cv.best_estimator_.named_steps.linearregression` to the name `imps` to then\n",
    "#create a pandas Series object of the feature importances, with the index given by the\n",
    "#training data column names, sorting the values in descending order\n",
    "selected = lr_grid_cv.best_estimator_.named_steps.selectkbest.get_support()\n",
    "plt.subplots(figsize=(10, 5))\n",
    "imps = lr_grid_cv.best_estimator_.named_steps.linearregression.coef_\n",
    "lr_feat_imps = pd.Series(imps, index=X_train.columns[selected]).sort_values(ascending=False)\n",
    "lr_feat_imps.plot(kind='bar')\n",
    "plt.xlabel('features')\n",
    "plt.ylabel('importance')\n",
    "plt.title('Best ranking differential features only linear regressor feature importances');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cd2562",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "So a simple linear model including only ranking and ranking derivative information outperformed the \"dummy\" model (RMSE,STD when applicable): \n",
    "\n",
    "* Dummy Model: 6.16% Train; 6.18% Test\n",
    "* Linear Model Using Ranking Data Only: 6.06% (.10%) Train; 6.12% Test\n",
    "\n",
    "* Also, not surprisingly based on both the EDA correlational analysis and intuition, the non-differential ranking feature failed to improve prediction above and beyond the player differential version. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8691d024",
   "metadata": {},
   "source": [
    "### Save Best Simple Linear Model Object From Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4531d3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the best linear model\n",
    "best_model = lr_grid_cv.best_estimator_\n",
    "best_model.version = '1.0'\n",
    "best_model.pandas_version = pd.__version__\n",
    "best_model.numpy_version = np.__version__\n",
    "best_model.sklearn_version = sklearn_version\n",
    "best_model.X_columns = [col for col in X_train.columns]\n",
    "best_model.build_datetime = datetime.datetime.now()\n",
    "\n",
    "modelpath = '../models'\n",
    "save_file(best_model, 'ranking_linearmodel_hard.pkl', modelpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4d7a4a",
   "metadata": {},
   "source": [
    "### Save Prediction Metrics from Dummy and Ranking-Based Linear Regression Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add1b519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save other data for model comparisons in machine learning model stage\n",
    "comp_data_from4 = (MAEs_dummy, MSEs_dummy, RMSEs_dummy, MAE_LR_train, MAE_LR_test, MSE_LR_train, MSE_LR_test, RMSE_LR_train, RMSE_LR_test)\n",
    "with open('../data/comp_data_from4_hard.pickle', 'wb') as f:\n",
    "    pickle.dump(comp_data_from4, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42db6fa9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
